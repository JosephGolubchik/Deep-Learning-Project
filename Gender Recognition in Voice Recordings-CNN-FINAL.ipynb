{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition in Voice Recordings\n",
    "##### Joseph Golubchik (209195353), Johann Thuillier (336104120), Shlomi Wenberger (203179403)\n",
    "\n",
    "The aim of our project is to use logistic regression to classify a persons emotional state from a recording of him speaking.  \n",
    "\n",
    "## Dataset\n",
    "The dataset we used is “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)”  \n",
    "https://zenodo.org/record/1188976  \n",
    "\n",
    "The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). We used only the speach files and not the song files, and used only the audio files and not the videos.\n",
    "\n",
    "Speech file contains 1440 files: 60 trials per actor x 24 actors = 1440. The labels for each file will be taken from the filenames: The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the features from data_xy\n",
    "def getXvalues(data_xy):\n",
    "    x_values = []\n",
    "    for data in data_xy:\n",
    "        x_values.append(data[0])\n",
    "    return x_values\n",
    "\n",
    "# Function to extract only the labels from data_xy\n",
    "def getYvalues(data_xy):\n",
    "    y_values = []\n",
    "    for data in data_xy:\n",
    "        y_values.append(data[1])\n",
    "    return y_values\n",
    "\n",
    "# Sigmoid function\n",
    "def logistic_fun(z):\n",
    "    return 1/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files = 1440 ,Number of actors = 24\n",
      "Number of train examples = 1007 ,Number of test examples = 433\n"
     ]
    }
   ],
   "source": [
    "# Loading the filenames from the folder with the audio files.\n",
    "filenames = []\n",
    "\n",
    "for i in range(1,25):\n",
    "    if (i < 10):\n",
    "        folderNum = \"0\"+str(i)\n",
    "    else:\n",
    "        folderNum = str(i)\n",
    "    for file in os.listdir('audio/Actor_'+folderNum):\n",
    "        filenames.append('Actor_'+folderNum+'/'+file)\n",
    "        \n",
    "# Shuffling the filenames array.\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Spliting the dataset into train and test files,\n",
    "# 70% train and 30% test.\n",
    "num_train = int(len(filenames)*0.7)\n",
    "num_test = len(filenames) - num_train\n",
    "\n",
    "print(\"Number of files =\",len(filenames),\",Number of actors =\",int(len(filenames)/60))\n",
    "print(\"Number of train examples =\",num_train,\",Number of test examples =\",num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 70.03877352358502 Seconds\n"
     ]
    }
   ],
   "source": [
    "data_x_train = []\n",
    "data_x_test = []\n",
    "data_y_train = []\n",
    "data_y_test = []\n",
    "\n",
    "# max_pad_len = 11\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# For each of the training examples,\n",
    "# extract from each file its Mel-frequency cepstral coefficients (MFCCs)\n",
    "# and append the mfccs to the array that stores the features of each train file - data_x_train.\n",
    "# look at the filename and create a label for the example,\n",
    "# Where the 8'th character determines the label.\n",
    "# Ex: filename[7] == 3 => label: [0,0,1,0,0,0,0,0]\n",
    "# Actor_13/03-01-05-01-01-01-13.wav\n",
    "for filename in filenames[:num_train]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_train.append(mfccs)\n",
    "    label = np.zeros(2)\n",
    "    label[int(filename[19])%2] = 1\n",
    "    data_y_train.append(label)\n",
    "    \n",
    "    np.save('saved/' + filename[9:-3] + str(np.argmax(label)) + '.npy', mfccs)\n",
    "    \n",
    "\n",
    "# Do the same for the testing examples.\n",
    "for filename in filenames[num_train:]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_test.append(mfccs)\n",
    "    label = np.zeros(2)\n",
    "    label[int(filename[19])%2] = 1\n",
    "    data_y_test.append(label)\n",
    "    \n",
    "    np.save('saved/' + filename[9:-3] + str(np.argmax(label)) + '.npy', mfccs)\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "print('Loading time:', stop_time - start_time, \"Seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new array that will contain tuples where the first element is the features of the example,\n",
    "# and the second element is the label of the example.\n",
    "# This is neccesary so we can shuffle the order of the examples around after each training epoch.\n",
    "data_xy_train = []\n",
    "for i in range(len(data_x_train)):\n",
    "#     # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "#     if len(data_x_train[i]) == 216:\n",
    "    temp_arr = np.copy(data_x_train[i])\n",
    "    temp_arr.resize(256)\n",
    "    data_xy_train.append( (temp_arr, data_y_train[i]) )\n",
    "    \n",
    "data_xy_test = []\n",
    "for i in range(len(data_x_test)):\n",
    "#     # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "#     if len(data_x_test[i]) == 216:\n",
    "    temp_arr = np.copy(data_x_test[i])\n",
    "    temp_arr.resize(256)\n",
    "    data_xy_test.append( (temp_arr, data_y_test[i]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.501155\n",
      "step 5, training accuracy 0.498845\n",
      "step 10, training accuracy 0.535797\n",
      "step 15, training accuracy 0.512702\n",
      "step 20, training accuracy 0.616628\n",
      "step 25, training accuracy 0.595843\n",
      "step 30, training accuracy 0.667436\n",
      "step 35, training accuracy 0.623557\n",
      "step 40, training accuracy 0.667436\n",
      "step 45, training accuracy 0.685912\n",
      "step 50, training accuracy 0.672055\n",
      "step 55, training accuracy 0.678984\n",
      "step 60, training accuracy 0.688222\n",
      "step 65, training accuracy 0.681293\n",
      "step 70, training accuracy 0.69746\n",
      "step 75, training accuracy 0.69746\n",
      "step 80, training accuracy 0.688222\n",
      "step 85, training accuracy 0.709007\n",
      "step 90, training accuracy 0.711316\n",
      "step 95, training accuracy 0.720554\n",
      "step 100, training accuracy 0.725173\n",
      "step 105, training accuracy 0.720554\n",
      "step 110, training accuracy 0.732102\n",
      "step 115, training accuracy 0.725173\n",
      "step 120, training accuracy 0.725173\n",
      "step 125, training accuracy 0.736721\n",
      "step 130, training accuracy 0.73903\n",
      "step 135, training accuracy 0.750577\n",
      "step 140, training accuracy 0.743649\n",
      "step 145, training accuracy 0.755196\n",
      "step 150, training accuracy 0.755196\n",
      "step 155, training accuracy 0.762125\n",
      "step 160, training accuracy 0.762125\n",
      "step 165, training accuracy 0.766744\n",
      "step 170, training accuracy 0.769053\n",
      "step 175, training accuracy 0.769053\n",
      "step 180, training accuracy 0.766744\n",
      "step 185, training accuracy 0.769053\n",
      "step 190, training accuracy 0.773672\n",
      "step 195, training accuracy 0.785219\n",
      "step 200, training accuracy 0.7806\n",
      "step 205, training accuracy 0.801386\n",
      "step 210, training accuracy 0.789838\n",
      "step 215, training accuracy 0.799076\n",
      "step 220, training accuracy 0.794457\n",
      "step 225, training accuracy 0.803695\n",
      "step 230, training accuracy 0.806005\n",
      "step 235, training accuracy 0.806005\n",
      "step 240, training accuracy 0.803695\n",
      "step 245, training accuracy 0.817552\n",
      "step 250, training accuracy 0.808314\n",
      "step 255, training accuracy 0.822171\n",
      "step 260, training accuracy 0.829099\n",
      "step 265, training accuracy 0.829099\n",
      "step 270, training accuracy 0.833718\n",
      "step 275, training accuracy 0.842956\n",
      "step 280, training accuracy 0.845266\n",
      "step 285, training accuracy 0.849885\n",
      "step 290, training accuracy 0.847575\n",
      "step 295, training accuracy 0.849885\n",
      "step 300, training accuracy 0.861432\n",
      "step 305, training accuracy 0.863741\n",
      "step 310, training accuracy 0.866051\n",
      "step 315, training accuracy 0.87067\n",
      "step 320, training accuracy 0.861432\n",
      "step 325, training accuracy 0.882217\n",
      "step 330, training accuracy 0.877598\n",
      "step 335, training accuracy 0.884527\n",
      "step 340, training accuracy 0.875289\n",
      "step 345, training accuracy 0.893764\n",
      "step 350, training accuracy 0.884527\n",
      "step 355, training accuracy 0.889145\n",
      "step 360, training accuracy 0.891455\n",
      "step 365, training accuracy 0.886836\n",
      "step 370, training accuracy 0.893764\n",
      "step 375, training accuracy 0.893764\n",
      "step 380, training accuracy 0.907621\n",
      "step 385, training accuracy 0.896074\n",
      "step 390, training accuracy 0.909931\n",
      "step 395, training accuracy 0.905312\n",
      "step 400, training accuracy 0.91455\n",
      "step 405, training accuracy 0.919169\n",
      "step 410, training accuracy 0.916859\n",
      "step 415, training accuracy 0.919169\n",
      "step 420, training accuracy 0.91455\n",
      "step 425, training accuracy 0.919169\n",
      "step 430, training accuracy 0.916859\n",
      "step 435, training accuracy 0.923788\n",
      "step 440, training accuracy 0.926097\n",
      "step 445, training accuracy 0.923788\n",
      "step 450, training accuracy 0.930716\n",
      "step 455, training accuracy 0.937644\n",
      "step 460, training accuracy 0.937644\n",
      "step 465, training accuracy 0.937644\n",
      "step 470, training accuracy 0.935335\n",
      "step 475, training accuracy 0.944573\n",
      "step 480, training accuracy 0.942263\n",
      "step 485, training accuracy 0.942263\n",
      "step 490, training accuracy 0.949192\n",
      "step 495, training accuracy 0.942263\n",
      "step 500, training accuracy 0.951501\n",
      "step 505, training accuracy 0.953811\n",
      "step 510, training accuracy 0.949192\n",
      "step 515, training accuracy 0.953811\n",
      "step 520, training accuracy 0.960739\n",
      "step 525, training accuracy 0.95843\n",
      "step 530, training accuracy 0.960739\n",
      "step 535, training accuracy 0.960739\n",
      "step 540, training accuracy 0.963049\n",
      "step 545, training accuracy 0.965358\n",
      "step 550, training accuracy 0.969977\n",
      "step 555, training accuracy 0.965358\n",
      "step 560, training accuracy 0.965358\n",
      "step 565, training accuracy 0.979215\n",
      "step 570, training accuracy 0.972286\n",
      "step 575, training accuracy 0.969977\n",
      "step 580, training accuracy 0.979215\n",
      "step 585, training accuracy 0.986143\n",
      "step 590, training accuracy 0.965358\n",
      "step 595, training accuracy 0.986143\n",
      "step 600, training accuracy 0.986143\n",
      "step 605, training accuracy 0.983834\n",
      "step 610, training accuracy 0.988453\n",
      "step 615, training accuracy 0.988453\n",
      "step 620, training accuracy 0.986143\n",
      "step 625, training accuracy 0.986143\n",
      "step 630, training accuracy 0.990762\n",
      "step 635, training accuracy 0.990762\n",
      "step 640, training accuracy 0.990762\n",
      "step 645, training accuracy 0.990762\n",
      "step 650, training accuracy 0.993072\n",
      "step 655, training accuracy 0.995381\n",
      "step 660, training accuracy 0.995381\n",
      "step 665, training accuracy 0.990762\n",
      "step 670, training accuracy 0.990762\n",
      "step 675, training accuracy 0.997691\n",
      "step 680, training accuracy 0.997691\n",
      "step 685, training accuracy 0.997691\n",
      "step 690, training accuracy 1\n",
      "step 695, training accuracy 1\n",
      "step 700, training accuracy 1\n",
      "step 705, training accuracy 0.995381\n",
      "step 710, training accuracy 1\n",
      "step 715, training accuracy 1\n",
      "step 720, training accuracy 1\n",
      "step 725, training accuracy 1\n",
      "step 730, training accuracy 1\n",
      "step 735, training accuracy 1\n",
      "step 740, training accuracy 1\n",
      "step 745, training accuracy 1\n",
      "step 750, training accuracy 1\n",
      "step 755, training accuracy 1\n",
      "step 760, training accuracy 1\n",
      "step 765, training accuracy 1\n",
      "step 770, training accuracy 1\n",
      "step 775, training accuracy 1\n",
      "step 780, training accuracy 1\n",
      "step 785, training accuracy 1\n",
      "step 790, training accuracy 1\n",
      "step 795, training accuracy 1\n",
      "step 800, training accuracy 1\n",
      "step 805, training accuracy 1\n",
      "step 810, training accuracy 1\n",
      "step 815, training accuracy 1\n",
      "step 820, training accuracy 1\n",
      "step 825, training accuracy 1\n",
      "step 830, training accuracy 1\n",
      "step 835, training accuracy 1\n",
      "step 840, training accuracy 1\n",
      "step 845, training accuracy 1\n",
      "step 850, training accuracy 1\n",
      "step 855, training accuracy 1\n",
      "step 860, training accuracy 1\n",
      "step 865, training accuracy 1\n",
      "step 870, training accuracy 1\n",
      "step 875, training accuracy 1\n",
      "step 880, training accuracy 1\n",
      "step 885, training accuracy 1\n",
      "step 890, training accuracy 1\n",
      "step 895, training accuracy 1\n",
      "step 900, training accuracy 1\n",
      "step 905, training accuracy 1\n",
      "step 910, training accuracy 1\n",
      "step 915, training accuracy 1\n",
      "step 920, training accuracy 1\n",
      "step 925, training accuracy 1\n",
      "step 930, training accuracy 1\n",
      "step 935, training accuracy 1\n",
      "step 940, training accuracy 1\n",
      "step 945, training accuracy 1\n",
      "step 950, training accuracy 1\n",
      "step 955, training accuracy 1\n",
      "step 960, training accuracy 1\n",
      "step 965, training accuracy 1\n",
      "step 970, training accuracy 1\n",
      "step 975, training accuracy 1\n",
      "step 980, training accuracy 1\n",
      "step 985, training accuracy 1\n",
      "step 990, training accuracy 1\n",
      "step 995, training accuracy 1\n",
      "step 1000, training accuracy 1\n",
      "step 1005, training accuracy 1\n",
      "step 1010, training accuracy 1\n",
      "step 1015, training accuracy 1\n",
      "step 1020, training accuracy 1\n",
      "step 1025, training accuracy 1\n",
      "step 1030, training accuracy 1\n",
      "step 1035, training accuracy 1\n",
      "step 1040, training accuracy 1\n",
      "step 1045, training accuracy 1\n",
      "step 1050, training accuracy 1\n",
      "step 1055, training accuracy 1\n",
      "step 1060, training accuracy 1\n",
      "step 1065, training accuracy 1\n",
      "step 1070, training accuracy 1\n",
      "step 1075, training accuracy 1\n",
      "step 1080, training accuracy 1\n",
      "step 1085, training accuracy 1\n",
      "step 1090, training accuracy 1\n",
      "step 1095, training accuracy 1\n",
      "step 1100, training accuracy 1\n",
      "step 1105, training accuracy 1\n",
      "step 1110, training accuracy 1\n",
      "step 1115, training accuracy 1\n",
      "step 1120, training accuracy 1\n",
      "step 1125, training accuracy 1\n",
      "step 1130, training accuracy 1\n",
      "step 1135, training accuracy 1\n",
      "step 1140, training accuracy 1\n",
      "step 1145, training accuracy 1\n",
      "step 1150, training accuracy 1\n",
      "step 1155, training accuracy 1\n",
      "step 1160, training accuracy 1\n",
      "step 1165, training accuracy 1\n",
      "step 1170, training accuracy 1\n",
      "step 1175, training accuracy 1\n",
      "step 1180, training accuracy 1\n",
      "step 1185, training accuracy 1\n",
      "step 1190, training accuracy 1\n",
      "step 1195, training accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200, training accuracy 1\n",
      "step 1205, training accuracy 1\n",
      "step 1210, training accuracy 1\n",
      "step 1215, training accuracy 1\n",
      "step 1220, training accuracy 1\n",
      "step 1225, training accuracy 1\n",
      "step 1230, training accuracy 1\n",
      "step 1235, training accuracy 1\n",
      "step 1240, training accuracy 1\n",
      "step 1245, training accuracy 1\n",
      "step 1250, training accuracy 1\n",
      "step 1255, training accuracy 1\n",
      "step 1260, training accuracy 1\n",
      "step 1265, training accuracy 1\n",
      "step 1270, training accuracy 1\n",
      "step 1275, training accuracy 1\n",
      "step 1280, training accuracy 1\n",
      "step 1285, training accuracy 1\n",
      "step 1290, training accuracy 1\n",
      "step 1295, training accuracy 1\n",
      "step 1300, training accuracy 1\n",
      "step 1305, training accuracy 1\n",
      "step 1310, training accuracy 1\n",
      "step 1315, training accuracy 1\n",
      "step 1320, training accuracy 1\n",
      "step 1325, training accuracy 1\n",
      "step 1330, training accuracy 1\n",
      "step 1335, training accuracy 1\n",
      "step 1340, training accuracy 1\n",
      "step 1345, training accuracy 1\n",
      "step 1350, training accuracy 1\n",
      "step 1355, training accuracy 1\n",
      "step 1360, training accuracy 1\n",
      "step 1365, training accuracy 1\n",
      "step 1370, training accuracy 1\n",
      "step 1375, training accuracy 1\n",
      "step 1380, training accuracy 1\n",
      "step 1385, training accuracy 1\n",
      "step 1390, training accuracy 1\n",
      "step 1395, training accuracy 1\n",
      "step 1400, training accuracy 1\n",
      "step 1405, training accuracy 1\n",
      "step 1410, training accuracy 1\n",
      "step 1415, training accuracy 1\n",
      "step 1420, training accuracy 1\n",
      "step 1425, training accuracy 1\n",
      "step 1430, training accuracy 1\n",
      "step 1435, training accuracy 1\n",
      "step 1440, training accuracy 1\n",
      "step 1445, training accuracy 1\n",
      "step 1450, training accuracy 1\n",
      "step 1455, training accuracy 1\n",
      "step 1460, training accuracy 1\n",
      "step 1465, training accuracy 1\n",
      "step 1470, training accuracy 1\n",
      "step 1475, training accuracy 1\n",
      "step 1480, training accuracy 1\n",
      "step 1485, training accuracy 1\n",
      "step 1490, training accuracy 1\n",
      "step 1495, training accuracy 1\n",
      "step 1500, training accuracy 1\n",
      "step 1505, training accuracy 1\n",
      "step 1510, training accuracy 1\n",
      "step 1515, training accuracy 1\n",
      "step 1520, training accuracy 1\n",
      "step 1525, training accuracy 1\n",
      "step 1530, training accuracy 1\n",
      "step 1535, training accuracy 1\n",
      "step 1540, training accuracy 1\n",
      "step 1545, training accuracy 1\n",
      "step 1550, training accuracy 1\n",
      "step 1555, training accuracy 1\n",
      "step 1560, training accuracy 1\n",
      "step 1565, training accuracy 1\n",
      "step 1570, training accuracy 1\n",
      "step 1575, training accuracy 1\n",
      "step 1580, training accuracy 1\n",
      "step 1585, training accuracy 1\n",
      "step 1590, training accuracy 1\n",
      "step 1595, training accuracy 1\n",
      "step 1600, training accuracy 1\n",
      "step 1605, training accuracy 1\n",
      "step 1610, training accuracy 1\n",
      "step 1615, training accuracy 1\n",
      "step 1620, training accuracy 1\n",
      "step 1625, training accuracy 1\n",
      "step 1630, training accuracy 1\n",
      "step 1635, training accuracy 1\n",
      "step 1640, training accuracy 1\n",
      "step 1645, training accuracy 1\n",
      "step 1650, training accuracy 1\n",
      "step 1655, training accuracy 1\n",
      "step 1660, training accuracy 1\n",
      "step 1665, training accuracy 1\n",
      "step 1670, training accuracy 1\n",
      "step 1675, training accuracy 1\n",
      "step 1680, training accuracy 1\n",
      "step 1685, training accuracy 1\n",
      "step 1690, training accuracy 1\n",
      "step 1695, training accuracy 1\n",
      "step 1700, training accuracy 1\n",
      "step 1705, training accuracy 1\n",
      "step 1710, training accuracy 1\n",
      "step 1715, training accuracy 1\n",
      "step 1720, training accuracy 1\n",
      "step 1725, training accuracy 1\n",
      "step 1730, training accuracy 1\n",
      "step 1735, training accuracy 1\n",
      "step 1740, training accuracy 1\n",
      "step 1745, training accuracy 1\n",
      "step 1750, training accuracy 1\n",
      "step 1755, training accuracy 1\n",
      "step 1760, training accuracy 1\n",
      "step 1765, training accuracy 1\n",
      "step 1770, training accuracy 1\n",
      "step 1775, training accuracy 1\n",
      "step 1780, training accuracy 1\n",
      "step 1785, training accuracy 1\n",
      "step 1790, training accuracy 1\n",
      "step 1795, training accuracy 1\n",
      "step 1800, training accuracy 1\n",
      "step 1805, training accuracy 1\n",
      "step 1810, training accuracy 1\n",
      "step 1815, training accuracy 1\n",
      "step 1820, training accuracy 1\n",
      "step 1825, training accuracy 1\n",
      "step 1830, training accuracy 1\n",
      "step 1835, training accuracy 1\n",
      "step 1840, training accuracy 1\n",
      "step 1845, training accuracy 1\n",
      "step 1850, training accuracy 1\n",
      "step 1855, training accuracy 1\n",
      "step 1860, training accuracy 1\n",
      "step 1865, training accuracy 1\n",
      "step 1870, training accuracy 1\n",
      "step 1875, training accuracy 1\n",
      "step 1880, training accuracy 1\n",
      "step 1885, training accuracy 1\n",
      "step 1890, training accuracy 1\n",
      "step 1895, training accuracy 1\n",
      "step 1900, training accuracy 1\n",
      "step 1905, training accuracy 1\n",
      "step 1910, training accuracy 1\n",
      "step 1915, training accuracy 1\n",
      "step 1920, training accuracy 1\n",
      "step 1925, training accuracy 1\n",
      "step 1930, training accuracy 1\n",
      "step 1935, training accuracy 1\n",
      "step 1940, training accuracy 1\n",
      "step 1945, training accuracy 1\n",
      "step 1950, training accuracy 1\n",
      "step 1955, training accuracy 1\n",
      "step 1960, training accuracy 1\n",
      "step 1965, training accuracy 1\n",
      "step 1970, training accuracy 1\n",
      "step 1975, training accuracy 1\n",
      "step 1980, training accuracy 1\n",
      "step 1985, training accuracy 1\n",
      "step 1990, training accuracy 1\n",
      "step 1995, training accuracy 1\n",
      "test accuracy 1\n",
      "runtime:  27.792224271394844\n"
     ]
    }
   ],
   "source": [
    "features = len(data_xy_train[0][0])\n",
    "labels = len(data_xy_train[0][1])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, labels], name=\"y_\")\n",
    "x_image = tf.reshape(x, [-1,16,16,1]) #if we had RGB, we would have 3 channels\n",
    "\n",
    "f1=1\n",
    "f2=32\n",
    "f3=64\n",
    "\n",
    "k1=3\n",
    "k2=5\n",
    "k3=5\n",
    "\n",
    "fc1_nodes=1024\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([k1, k1, 1, f1], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[f1]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([k2, k2, f1, f2], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[f2]))\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv3 = tf.Variable(tf.truncated_normal([k3, k3, f2, f3], stddev=0.1))\n",
    "b_conv3 = tf.Variable(tf.constant(0.1, shape=[f3]))\n",
    "h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 2*2*f3])\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([2 * 2 * f3, fc1_nodes], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[fc1_nodes]))\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([fc1_nodes, labels], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[labels]))\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(2000):\n",
    "    if i % 5 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test), keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                               \n",
    "    train_step.run(feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test), keep_prob: 0.5})\n",
    "    random.shuffle(data_xy_train)\n",
    "\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "                               \n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test), keep_prob: 1.0}))\n",
    "print('runtime: ', stop_time - start_time)  \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
