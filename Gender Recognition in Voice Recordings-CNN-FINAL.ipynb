{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition in Voice Recordings\n",
    "##### Joseph Golubchik (209195353), Johann Thuillier (336104120), Shlomi Wenberger (203179403)\n",
    "\n",
    "The aim of our project is to use logistic regression to classify a persons emotional state from a recording of him speaking.  \n",
    "\n",
    "## Dataset\n",
    "The dataset we used is “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)”  \n",
    "https://zenodo.org/record/1188976  \n",
    "\n",
    "The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). We used only the speach files and not the song files, and used only the audio files and not the videos.\n",
    "\n",
    "Speech file contains 1440 files: 60 trials per actor x 24 actors = 1440. The labels for each file will be taken from the filenames: The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the features from data_xy\n",
    "def getXvalues(data_xy):\n",
    "    x_values = []\n",
    "    for data in data_xy:\n",
    "        x_values.append(data[0])\n",
    "    return x_values\n",
    "\n",
    "# Function to extract only the labels from data_xy\n",
    "def getYvalues(data_xy):\n",
    "    y_values = []\n",
    "    for data in data_xy:\n",
    "        y_values.append(data[1])\n",
    "    return y_values\n",
    "\n",
    "# Sigmoid function\n",
    "def logistic_fun(z):\n",
    "    return 1/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files = 1440 ,Number of actors = 24\n",
      "Number of train examples = 1007 ,Number of test examples = 433\n"
     ]
    }
   ],
   "source": [
    "# Loading the filenames from the folder with the audio files.\n",
    "filenames = []\n",
    "\n",
    "for i in range(1,25):\n",
    "    if (i < 10):\n",
    "        folderNum = \"0\"+str(i)\n",
    "    else:\n",
    "        folderNum = str(i)\n",
    "    for file in os.listdir('audio/Actor_'+folderNum):\n",
    "        filenames.append('Actor_'+folderNum+'/'+file)\n",
    "        \n",
    "# Shuffling the filenames array.\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Spliting the dataset into train and test files,\n",
    "# 70% train and 30% test.\n",
    "num_train = int(len(filenames)*0.7)\n",
    "num_test = len(filenames) - num_train\n",
    "\n",
    "print(\"Number of files =\",len(filenames),\",Number of actors =\",int(len(filenames)/60))\n",
    "print(\"Number of train examples =\",num_train,\",Number of test examples =\",num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 117.84399590718388 Seconds\n"
     ]
    }
   ],
   "source": [
    "data_x_train = []\n",
    "data_x_test = []\n",
    "data_y_train = []\n",
    "data_y_test = []\n",
    "\n",
    "# max_pad_len = 11\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# For each of the training examples,\n",
    "# extract from each file its Mel-frequency cepstral coefficients (MFCCs)\n",
    "# and append the mfccs to the array that stores the features of each train file - data_x_train.\n",
    "# look at the filename and create a label for the example,\n",
    "# Where the 8'th character determines the label.\n",
    "# Ex: filename[7] == 3 => label: [0,0,1,0,0,0,0,0]\n",
    "# Actor_13/03-01-05-01-01-01-13.wav\n",
    "for filename in filenames[:num_train]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_train.append(mfccs)\n",
    "    label = np.zeros(2)\n",
    "    label[int(filename[19])%2] = 1\n",
    "    data_y_train.append(label)\n",
    "    \n",
    "    np.save('saved/' + filename[9:-3] + str(np.argmax(label)) + '.npy', mfccs)\n",
    "    \n",
    "\n",
    "# Do the same for the testing examples.\n",
    "for filename in filenames[num_train:]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_test.append(mfccs)\n",
    "    label = np.zeros(2)\n",
    "    label[int(filename[19])%2] = 1\n",
    "    data_y_test.append(label)\n",
    "    \n",
    "    np.save('saved/' + filename[9:-3] + str(np.argmax(label)) + '.npy', mfccs)\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "print('Loading time:', stop_time - start_time, \"Seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new array that will contain tuples where the first element is the features of the example,\n",
    "# and the second element is the label of the example.\n",
    "# This is neccesary so we can shuffle the order of the examples around after each training epoch.\n",
    "data_xy_train = []\n",
    "for i in range(len(data_x_train)):\n",
    "#     # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "#     if len(data_x_train[i]) == 216:\n",
    "    temp_arr = np.copy(data_x_train[i])\n",
    "    temp_arr.resize(256)\n",
    "    data_xy_train.append( (temp_arr, data_y_train[i]) )\n",
    "    \n",
    "data_xy_test = []\n",
    "for i in range(len(data_x_test)):\n",
    "#     # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "#     if len(data_x_test[i]) == 216:\n",
    "    temp_arr = np.copy(data_x_test[i])\n",
    "    temp_arr.resize(256)\n",
    "    data_xy_test.append( (temp_arr, data_y_test[i]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.548163\n",
      "step 100, training accuracy 0.695134\n",
      "step 200, training accuracy 0.704072\n",
      "step 300, training accuracy 0.712016\n",
      "test accuracy 0.713626\n",
      "runtime:  6.696976727550918\n"
     ]
    }
   ],
   "source": [
    "features = len(data_xy_train[0][0])\n",
    "labels = len(data_xy_train[0][1])\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, features], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, labels], name=\"y_\")\n",
    "x_image = tf.reshape(x, [-1,16,16,1]) #if we had RGB, we would have 3 channels\n",
    "\n",
    "f1=1\n",
    "f2=32\n",
    "f3=64\n",
    "\n",
    "k1=5\n",
    "k2=5\n",
    "k3=5\n",
    "\n",
    "fc1_nodes=1024\n",
    "\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([k1, k1, 1, f1], stddev=0.1))\n",
    "b_conv1 = tf.Variable(tf.constant(0.1, shape=[f1]))\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1)\n",
    "h_pool1 = tf.nn.max_pool(h_conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([k2, k2, f1, f2], stddev=0.1))\n",
    "b_conv2 = tf.Variable(tf.constant(0.1, shape=[f2]))\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2)\n",
    "h_pool2 = tf.nn.max_pool(h_conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "W_conv3 = tf.Variable(tf.truncated_normal([k3, k3, f2, f3], stddev=0.1))\n",
    "b_conv3 = tf.Variable(tf.constant(0.1, shape=[f3]))\n",
    "h_conv3 = tf.nn.relu(tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3)\n",
    "h_pool3 = tf.nn.max_pool(h_conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "h_pool3_flat = tf.reshape(h_pool3, [-1, 2*2*f3])\n",
    "\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([2 * 2 * f3, fc1_nodes], stddev=0.1))\n",
    "b_fc1 = tf.Variable(tf.constant(0.1, shape=[fc1_nodes]))\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "W_fc2 = tf.Variable(tf.truncated_normal([fc1_nodes, labels], stddev=0.1))\n",
    "b_fc2 = tf.Variable(tf.constant(0.1, shape=[labels]))\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(400):\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:getXvalues(data_xy_train), y_:getYvalues(data_xy_train), keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "                               \n",
    "    train_step.run(feed_dict={x:getXvalues(data_xy_train), y_:getYvalues(data_xy_train), keep_prob: 0.5})\n",
    "    random.shuffle(data_xy_train)\n",
    "\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "                               \n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test), keep_prob: 1.0}))\n",
    "print('runtime: ', stop_time - start_time)  \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
