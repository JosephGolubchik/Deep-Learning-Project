{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition in Voice Recordings\n",
    "The aim of our project is to use logistic regression to classify a persons emotional state from a recording of him speaking.  \n",
    "\n",
    "## Dataset\n",
    "The dataset we used is “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)”  \n",
    "https://zenodo.org/record/1188976  \n",
    "\n",
    "The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). We used only the speach files and not the song files, and used only the audio files and not the videos.\n",
    "\n",
    "Speech file contains 1440 files: 60 trials per actor x 24 actors = 1440. The labels for each file will be taken from the filenames: The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt - Distinguishing Between Happy and Sad Recordings\n",
    "At first we tried logistic regression with only two classes - happy and sad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files = 600 ,Number of actors = 10\n",
      "Number of train examples = 420 ,Number of test examples = 180\n"
     ]
    }
   ],
   "source": [
    "# Loading the filenames from the folder with the audio files.\n",
    "filenames = []\n",
    "for file in os.listdir('audio'):\n",
    "    filenames.append(file)\n",
    "\n",
    "# Shuffling the filenames array.\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Spliting the dataset into train and test files,\n",
    "# 70% train and 30% test.\n",
    "num_train = int(len(filenames)*0.7)\n",
    "num_test = len(filenames) - num_train\n",
    "\n",
    "print(\"Number of files =\",len(filenames),\",Number of actors =\",int(len(filenames)/60))\n",
    "print(\"Number of train examples =\",num_train,\",Number of test examples =\",num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 7.687271296661947 Seconds\n"
     ]
    }
   ],
   "source": [
    "data_x_train = []\n",
    "data_x_test = []\n",
    "data_y_train = []\n",
    "data_y_test = []\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# For each of the training examples,\n",
    "# extract from each file its Mel-frequency cepstral coefficients (MFCCs)\n",
    "# and append the mfccs to the array that stores the features of each train file - data_x_train.\n",
    "# look at the filename and create a label for the example,\n",
    "# Where the 8'th character is '3' if the file is a sad recording and '4' if it's a happy recording.\n",
    "for filename in filenames[:num_train]:\n",
    "    if filename[7] == '3':\n",
    "        data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "        sampling_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "        data_x_train.append(mfccs)\n",
    "        data_y_train.append(1)\n",
    "    elif filename[7] == '4':\n",
    "        data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "        sampling_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "        data_x_train.append(mfccs)\n",
    "        data_y_train.append(0)\n",
    "\n",
    "# Do the same for the testing examples.\n",
    "for filename in filenames[num_train:]:\n",
    "    if filename[7] == '3':\n",
    "        data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "        sampling_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "        data_x_test.append(mfccs)\n",
    "        data_y_test.append(1)\n",
    "    elif filename[7] == '4':\n",
    "        data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "        sampling_rate = np.array(sampling_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "        data_x_test.append(mfccs)\n",
    "        data_y_test.append(0)\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "print('Loading time:', stop_time - start_time, \"Seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow requires the y array that it gets to be of the shape (none, 1)\n",
    "# This converts our data_y arrays from the shape (none, ) to the required shape (none, 1)\n",
    "# ex: [0, 1, 1, 1, 0] => [[0], [1], [1], [1], [0]]\n",
    "data_y_train_correct = []\n",
    "data_y_test_correct = []\n",
    "\n",
    "for val in data_y_train:\n",
    "    val_arr = []\n",
    "    val_arr.append(val)\n",
    "    data_y_train_correct.append(val_arr)\n",
    "    \n",
    "for val in data_y_test:\n",
    "    val_arr = []\n",
    "    val_arr.append(val)\n",
    "    data_y_test_correct.append(val_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new array that will contain tuples where the first element is the features of the example,\n",
    "# and the second element is the label of the example.\n",
    "# This is neccesary so we can shuffle the order of the examples around after each training epoch.\n",
    "data_xy_train = []\n",
    "for i in range(len(data_x_train)):\n",
    "    data_xy_train.append( (data_x_train[i], data_y_train_correct[i]) )\n",
    "    \n",
    "data_xy_test = []\n",
    "for i in range(len(data_x_test)):\n",
    "    data_xy_test.append( (data_x_test[i], data_y_test_correct[i]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the features from data_xy\n",
    "def getXvalues(data_xy):\n",
    "    x_values = []\n",
    "    for data in data_xy:\n",
    "        x_values.append(data[0])\n",
    "    return x_values\n",
    "\n",
    "# Function to extract only the labels from data_xy\n",
    "def getYvalues(data_xy):\n",
    "    y_values = []\n",
    "    for data in data_xy:\n",
    "        y_values.append(data[1])\n",
    "    return y_values\n",
    "\n",
    "# Sigmoid function\n",
    "def logistic_fun(z):\n",
    "    return 1/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime:  0.14309279376493578\n"
     ]
    }
   ],
   "source": [
    "features = len(data_xy_train[0][0])\n",
    "eps = 1e-12\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "W = tf.Variable(tf.zeros([features,1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = tf.nn.sigmoid(tf.matmul(x,W) + b)\n",
    "\n",
    "loss = -tf.reduce_mean(y_*tf.log(y))\n",
    "update = tf.train.GradientDescentOptimizer(0.00001).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "loss_history_train = []\n",
    "loss_history_test = []\n",
    "accuracy_history = []\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(0,10):\n",
    "    sess.run(update, feed_dict = {x:getXvalues(data_xy_train), y_:getYvalues(data_xy_train)}) #BGD\n",
    "    train_loss = sess.run(loss, feed_dict = {x:getXvalues(data_xy_train), y_:getYvalues(data_xy_train)})\n",
    "    loss_history_train.append(train_loss)\n",
    "    random.shuffle(data_xy_train)\n",
    "    \n",
    "    if i%10 == 0:\n",
    "        right = 0\n",
    "        for i in range(len(data_x_test)):\n",
    "            test_loss = sess.run(loss, feed_dict = {x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test)})\n",
    "            loss_history_test.append(np.mean(sess.run(loss1, feed_dict = {x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test)})))\n",
    "            pred = logistic_fun(np.matmul(getXvalues(data_xy_train)[i],sess.run(W)) + sess.run(b))\n",
    "\n",
    "            if data_y_test[i] == 0 and pred < 0.5:\n",
    "                right += 1\n",
    "            if data_y_test[i] == 1 and pred > 0.5:\n",
    "                right += 1\n",
    "        accuracy_history.append(right/num_test)\n",
    "\n",
    "stop_time = timeit.default_timer()\n",
    "print('runtime: ', stop_time - start_time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
