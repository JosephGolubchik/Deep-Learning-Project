{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> Emotion Recognition in Voice Recordings </h1>\n",
    "<h5 style=\"text-align: center;\"> Joseph Golubchik (209195353), Johann Thuillier (336104120), Shlomi Wenberger (203179403) </h5>\n",
    "\n",
    "<h3 style=\"text-align: center;\"> Project Description </h3>\n",
    "The aim of our project is to use machine learning to classify a persons emotional state from a recording of him speaking.\n",
    "\n",
    "<h3 style=\"text-align: center;\"> Dataset </h3>\n",
    "The dataset we used is “The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)”  \n",
    "https://zenodo.org/record/1188976  \n",
    "\n",
    "The database contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression. All conditions are available in three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). We used only the speach files and not the song files, and used only the audio files and not the videos.\n",
    "\n",
    "Speech file contains 1440 files: 60 trials per actor x 24 actors = 1440. The labels for each file will be taken from the filenames: The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics: Filename identifiers Modality (01 = full-AV, 02 = video-only, 03 = audio-only). Vocal channel (01 = speech, 02 = song). Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised). Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion. Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\"). Repetition (01 = 1st repetition, 02 = 2nd repetition). Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of MLP](https://raw.githubusercontent.com/ledell/sldm4-h2o/master/mlp_network.png)\n",
    "<h2 style=\"text-align: center;\"> Multi Layered Perceptron </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract only the features from data_xy\n",
    "def getXvalues(data_xy):\n",
    "    x_values = []\n",
    "    for data in data_xy:\n",
    "        x_values.append(data[0])\n",
    "    return x_values\n",
    "\n",
    "# Function to extract only the labels from data_xy\n",
    "def getYvalues(data_xy):\n",
    "    y_values = []\n",
    "    for data in data_xy:\n",
    "        y_values.append(data[1])\n",
    "    return y_values\n",
    "\n",
    "# Sigmoid function\n",
    "def logistic_fun(z):\n",
    "    return 1/(1.0 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files = 1440 ,Number of actors = 24\n",
      "Number of train examples = 1007 ,Number of test examples = 433\n"
     ]
    }
   ],
   "source": [
    "# Loading the filenames from the folder with the audio files.\n",
    "filenames = []\n",
    "\n",
    "for i in range(1,25):\n",
    "    if (i < 10):\n",
    "        folderNum = \"0\"+str(i)\n",
    "    else:\n",
    "        folderNum = str(i)\n",
    "    for file in os.listdir('audio/Actor_'+folderNum):\n",
    "        filenames.append('Actor_'+folderNum+'/'+file)\n",
    "        \n",
    "# Shuffling the filenames array.\n",
    "random.shuffle(filenames)\n",
    "\n",
    "# Spliting the dataset into train and test files,\n",
    "# 70% train and 30% test.\n",
    "num_train = int(len(filenames)*0.7)\n",
    "num_test = len(filenames) - num_train\n",
    "\n",
    "print(\"Number of files =\",len(filenames),\",Number of actors =\",int(len(filenames)/60))\n",
    "print(\"Number of train examples =\",num_train,\",Number of test examples =\",num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading time: 177.9082642735782 Seconds\n"
     ]
    }
   ],
   "source": [
    "data_x_train = []\n",
    "data_x_test = []\n",
    "data_y_train = []\n",
    "data_y_test = []\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# For each of the training examples,\n",
    "# extract from each file its Mel-frequency cepstral coefficients (MFCCs)\n",
    "# and append the mfccs to the array that stores the features of each train file - data_x_train.\n",
    "# look at the filename and create a label for the example,\n",
    "# Where the 8'th character determines the label.\n",
    "# Ex: filename[7] == 3 => label: [0,0,1,0,0,0,0,0]\n",
    "for filename in filenames[:num_train]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_train.append(mfccs)\n",
    "    label = np.zeros(8)\n",
    "    label[int(filename[16])-1] = 1\n",
    "    data_y_train.append(label)\n",
    "\n",
    "# Do the same for the testing examples.\n",
    "for filename in filenames[num_train:]:\n",
    "    data, sampling_rate = librosa.load(\"audio/\" + filename, sr=22050*2, res_type='kaiser_fast', duration=2.5, offset=0.5)\n",
    "    sampling_rate = np.array(sampling_rate)\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=data, sr=sampling_rate, n_mfcc=13), axis=0)\n",
    "    data_x_test.append(mfccs)\n",
    "    label = np.zeros(8)\n",
    "    label[int(filename[16])-1] = 1\n",
    "    data_y_test.append(label)\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "print('Loading time:', stop_time - start_time, \"Seconds\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1007,) (433, 216)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data_x_train), np.shape(data_x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new array that will contain tuples where the first element is the features of the example,\n",
    "# and the second element is the label of the example.\n",
    "# This is neccesary so we can shuffle the order of the examples around after each training epoch.\n",
    "data_xy_train = []\n",
    "for i in range(len(data_x_train)):\n",
    "    # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "    if len(data_x_train[i]) == 216:\n",
    "        data_xy_train.append( (data_x_train[i], data_y_train[i]) )\n",
    "    \n",
    "data_xy_test = []\n",
    "for i in range(len(data_x_test)):\n",
    "    # For all but two of our files, our mfccs extraction returns 216 features, so we don't use these two.\n",
    "    if len(data_x_test[i]) == 216:\n",
    "        data_xy_test.append( (data_x_test[i], data_y_test[i]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005, 216) (433, 216)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(getXvalues(data_xy_train)), np.shape(getXvalues(data_xy_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Accuracy: 0.16859123\n",
      "Epoch 200: Accuracy: 0.11316397\n",
      "Epoch 400: Accuracy: 0.106235564\n",
      "Epoch 600: Accuracy: 0.1039261\n",
      "Epoch 800: Accuracy: 0.10161663\n",
      "Epoch 1000: Accuracy: 0.10161663\n",
      "Epoch 1200: Accuracy: 0.1039261\n",
      "Epoch 1400: Accuracy: 0.1039261\n",
      "Epoch 1600: Accuracy: 0.10161663\n",
      "Epoch 1800: Accuracy: 0.1039261\n",
      "Epoch 2000: Accuracy: 0.10161663\n",
      "Epoch 2200: Accuracy: 0.10161663\n",
      "Epoch 2400: Accuracy: 0.1039261\n",
      "Epoch 2600: Accuracy: 0.1039261\n",
      "Epoch 2800: Accuracy: 0.09006929\n",
      "Epoch 3000: Accuracy: 0.09237875\n",
      "Epoch 3200: Accuracy: 0.11778291\n",
      "Epoch 3400: Accuracy: 0.12702079\n",
      "Epoch 3600: Accuracy: 0.12702079\n",
      "Epoch 3800: Accuracy: 0.13163972\n",
      "Epoch 4000: Accuracy: 0.13394919\n",
      "Epoch 4200: Accuracy: 0.13625866\n",
      "Epoch 4400: Accuracy: 0.13394919\n",
      "Epoch 4600: Accuracy: 0.13394919\n",
      "Epoch 4800: Accuracy: 0.13625866\n",
      "Epoch 5000: Accuracy: 0.13856813\n",
      "Epoch 5200: Accuracy: 0.1408776\n",
      "Epoch 5400: Accuracy: 0.14549653\n",
      "Epoch 5600: Accuracy: 0.15704387\n",
      "Epoch 5800: Accuracy: 0.15704387\n",
      "Epoch 6000: Accuracy: 0.15473442\n",
      "Epoch 6200: Accuracy: 0.18937644\n",
      "Epoch 6400: Accuracy: 0.19168591\n",
      "Epoch 6600: Accuracy: 0.19168591\n",
      "Epoch 6800: Accuracy: 0.20323326\n",
      "Epoch 7000: Accuracy: 0.21016166\n",
      "Epoch 7200: Accuracy: 0.21247113\n",
      "Epoch 7400: Accuracy: 0.21709007\n",
      "Epoch 7600: Accuracy: 0.21939954\n",
      "Epoch 7800: Accuracy: 0.2147806\n",
      "Epoch 8000: Accuracy: 0.21709007\n",
      "Epoch 8200: Accuracy: 0.22170901\n",
      "Epoch 8400: Accuracy: 0.22170901\n",
      "Epoch 8600: Accuracy: 0.21939954\n",
      "Epoch 8800: Accuracy: 0.21247113\n",
      "Epoch 9000: Accuracy: 0.21247113\n",
      "Epoch 9200: Accuracy: 0.21247113\n",
      "Epoch 9400: Accuracy: 0.21016166\n",
      "Epoch 9600: Accuracy: 0.2078522\n",
      "Epoch 9800: Accuracy: 0.20554273\n",
      "Accuracy: 0.20323326\n",
      "runtime:  86.16924733153974\n"
     ]
    }
   ],
   "source": [
    "features = len(data_xy_train[0][0])\n",
    "hidden_layer_nodes = 10\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, features])\n",
    "y_ = tf.placeholder(tf.float32, [None, 8])\n",
    "W1 = tf.Variable(tf.truncated_normal([features,hidden_layer_nodes], stddev=0.1))\n",
    "b1 = tf.Variable(tf.constant(0.1, shape=[hidden_layer_nodes]))\n",
    "z1 = tf.add(tf.matmul(x,W1),b1)\n",
    "a1 = tf.nn.relu(z1)\n",
    "W2 = tf.Variable(tf.truncated_normal([hidden_layer_nodes,8], stddev=0.1))\n",
    "b2 = tf.Variable(0.)\n",
    "z2 = tf.matmul(a1,W2) + b2\n",
    "y = tf.nn.softmax(z2)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1])+0.1*tf.nn.l2_loss(W))\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i in range(10000):\n",
    "    sess.run(train_step, feed_dict={x:getXvalues(data_xy_train), y_:getYvalues(data_xy_train)})\n",
    "    if i % 200 == 0:\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        print('Epoch '+str(i)+':', \"Accuracy:\", sess.run(accuracy, feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test)}))\n",
    "#     random.shuffle(data_xy_train)\n",
    "    \n",
    "stop_time = timeit.default_timer()\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Accuracy:\", sess.run(accuracy, feed_dict={x:getXvalues(data_xy_test), y_:getYvalues(data_xy_test)}))\n",
    "print('runtime: ', stop_time - start_time)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
